{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDENTIFICATION OF NOVEL CLASSES OF NEOANTIGENS IN CANCER | Translatable ORFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data preparation\n",
    "\n",
    "This first cell should be modified according to the data that is going to be used. It is only available for datasets with paired samples per patient: normal and tumor. \n",
    "\n",
    "The **PROJECT** variable should be changed according to the GEO identifier.\n",
    "\n",
    "From the GEO website, the *SRR_Acc_List.txt* and *SraRunTable.txt* files should be manually downloaded and save in a directory. This directory should be specified in **SRR** variable.\n",
    "\n",
    "The pipeline is developed with the intention of running the most computationally expensive programs in a cluster. \n",
    "In this case, a Gluster File System has been used. The code to run on a cluster may need to be adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os,re,shutil,glob,openpyxl\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from gtfparse import read_gtf\n",
    "from matplotlib_venn import venn2, venn2_circles, venn2_unweighted\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "PROJECT=\"GSE193567\"\n",
    "\n",
    "DIR=os.path.join(\"data\",PROJECT)\n",
    "\n",
    "try:\n",
    "    os.makedirs(DIR) #path where to store all the itermediate steps and outputs of the pipeline\n",
    "except:\n",
    "    print(\"Directory for %s already exists\" %PROJECT)\n",
    "    \n",
    "CLUSTERDIR=\"/users/genomics/marta\" #path where to run and store things that run in a cluster\n",
    "SRR=\"/projects_eg/datasets/\"+PROJECT # path where SRR_Acc_List.txt and SraRunTable.txt are stored. It should be inside a folder named with GEO accession\n",
    "SRR_ACC=os.path.join(SRR,\"SRR_Acc_List.txt\") \n",
    "SRA=os.path.join(SRR,\"SraRunTable.txt\")\n",
    "\n",
    "FASTQDIR=os.path.join(DIR,\"fastq_files\") #path where to store fastq files\n",
    "try:\n",
    "    os.mkdir(FASTQDIR)\n",
    "except:\n",
    "    print(\"Fastq_files directory exists\")\n",
    "    \n",
    "shutil.copy(SRR_ACC, os.path.join(FASTQDIR,\"SRR_Acc_List.txt\"))\n",
    "shutil.copy(SRA, os.path.join(FASTQDIR,\"SraRunTable.txt\"))\n",
    "\n",
    "GENOMEDIR=\"genomes\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.path.join(DIR,\"analysis\"))\n",
    "    os.makedirs(os.path.join(DIR,\"results\"))\n",
    "    #os.makedirs(os.path.join(DIR,\"scripts\"))\n",
    "except:\n",
    "    print(\"Directory exists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "require(tidyr)\n",
    "require(dplyr)\n",
    "require(rtracklayer)\n",
    "#library(purrr)\n",
    "require(ggplot2)\n",
    "require(RColorBrewer)\n",
    "require(devtools)\n",
    "require(stringr)\n",
    "require(edgeR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a three column file with patient_id normal_id tumor_id for latter usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(os.path.join(FASTQDIR.split(\"/fastq_files\")[0],\"SraRunTable.txt\"))\n",
    "metadata = metadata[['Run','Individual','tissue']]\n",
    "\n",
    "normal = metadata[metadata['tissue'] == \"non-tumor\"]\n",
    "normal = normal[['Individual','Run']]\n",
    "\n",
    "tumor = metadata[metadata['tissue'] == \"tumor\"]\n",
    "tumor = tumor[['Individual','Run']].rename(columns ={'Run' : 'Run_t'})\n",
    "\n",
    "patients = pd.merge(normal, tumor, on=['Individual'])\n",
    "patients['Individual'] = patients['Individual'].str.split(' ').str[1]\n",
    "patients.to_csv(os.path.join(DIR,\"results/patients.csv\"),index=False, header=False)\n",
    "patients_summary = os.path.join(DIR,\"results/patients.csv\")\n",
    "\n",
    "patients_id=list(patients.iloc[:,0])\n",
    "normal_id=list(patients.iloc[:,1])\n",
    "tumor_id=list(patients.iloc[:,2])\n",
    "\n",
    "patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09.CIPHER\n",
    "\n",
    "CIPHER is a program for the prediction of coding sequences in transcripts. It calculates the coding score of each open reading frame (ORF) using a metric based on hexanucleotide frequencies (see Additional Information). It is specially well-suited to discover new small translated proteins.\n",
    "\n",
    "http://evolutionarygenomics.imim.es/cipher\n",
    "\n",
    "https://github.com/jorruior/CIPHER/blob/master/cipher.py\n",
    "\n",
    "We want to predict translatable open reading frames in both non-canonical regions (lncRNA and processed pseudogenes) and novel transcripts.\n",
    "\n",
    "**CIPHER NOCDS selected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$DIR\" \"$patients_summary\"\n",
    "\n",
    "module load Python/2.7.11\n",
    "\n",
    "cat $2 | while IFS=, read p normal tumor; do\n",
    "    INDIR=$1/analysis/08_tumor_specific/${p}\n",
    "    OUTPUT=$1/analysis/09_CIPHER/$p\n",
    "    mkdir $OUTPUT\n",
    "    #predict the longest ORF\n",
    "    python /genomics/users/marta/tools/CIPHER-master/cipher.py -i $INDIR/${p}_known_tumor_specific_genes_1FPKM_300kb_NOCDS_selected_gene.fa -o $OUTPUT/${p}_known_tumor_specific_genes_1FPKM_300kb_NOCDS_selected_8aa_cipher_longest -s human -x /genomics/users/marta/tools/CIPHER-master/tables/hsa_coding_to_intron_dicodon_usage.obj -t 8\n",
    "    #predict all ORFs\n",
    "    python /genomics/users/marta/tools/CIPHER-master/cipher.py -n all -i $INDIR/${p}_known_tumor_specific_genes_1FPKM_300kb_NOCDS_selected_gene.fa -o $OUTPUT/${p}_known_tumor_specific_genes_1FPKM_300kb_NOCDS_selected_8aa_cipher_all -s human -x /genomics/users/marta/tools/CIPHER-master/tables/hsa_coding_to_intron_dicodon_usage.obj -t 8\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate list file with ENST of NOCDS selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in patients_id:\n",
    "    INDIR=DIR+\"/analysis/09_CIPHER/\"+p \n",
    "    filename= p + \"_known_tumor_specific_genes_1FPKM_300kb_NOCDS_selected_8aa_cipher_longest_orfs.fa\" #each identifier only once\n",
    "    file = os.path.join(INDIR, filename)\n",
    "    outname= p + \"_NOCDS_id_list_FPKM1.csv\"\n",
    "    outfile= os.path.join(INDIR,outname)\n",
    "    with open(outfile, 'w') as out:\n",
    "        with open(file) as fasta_file:  \n",
    "            identifiers = dict()\n",
    "            out.write('transcript_id,transcript_type,transcript_name\\n')\n",
    "            for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)            \n",
    "                m = re.search(r'.*transcript_type=(.*);.*', seq_record.id)\n",
    "                m2=re.search(r'.*transcript_name=(.*)_.*', seq_record.id)\n",
    "                identifiers[seq_record.id[0:15]] = m.group(1) + \",\" + m2.group(1) #considering ENST identifiers have 15 characters\n",
    "\n",
    "\n",
    "        for ENST, type in identifiers.items():\n",
    "            out.write('%s,%s\\n' %(ENST,type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the NOCDS, compare with `nuORFdb` \n",
    "\n",
    "Ouspenskaia T, Law T, Clauser KR, Klaeger S, Sarkizova S, Aguet F, Li B, Christian E, Knisbacher BA, Le PM, Hartigan CR, Keshishian H, Apffel A, Oliveira G, Zhang W, Chen S, Chow YT, Ji Z, Jungreis I, Shukla SA, Justesen S, Bachireddy P, Kellis M, Getz G, Hacohen N, Keskin DB, Carr SA, Wu CJ, Regev A. Unannotated proteins expand the MHC-I-restricted immunopeptidome in cancer. Nat Biotechnol. 2022 Feb;40(2):209-217. doi: 10.1038/s41587-021-01021-3. Epub 2021 Oct 18. PMID: 34663921.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nuORFdb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$GENOMEDIR\"\n",
    "\n",
    "mkdir $1/nuORFdb\n",
    "cd $1/nuORFdb\n",
    "\n",
    "wget https://ftp.ncbi.nlm.nih.gov/geo/series/GSE143nnn/GSE143263/suppl/GSE143263_nuORFdb_v1.0.bed.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuORFdb = os.path.join(GENOMEDIR,\"nuORFdb/GSE143263_nuORFdb_v1.0.bed\")\n",
    "df = pd.read_csv(nuORFdb, sep=\"\\t\", header=None)\n",
    "nuORFdb_identifiers = df.iloc[:,3].str[:15]\n",
    "nuORFdb_identifiers.to_csv(nuORFdb_list, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select those lncRNA and pseudogenes we considered with coidng potential and also did Ouspenskaia et al. The ORFs does not need to be the same. It's just orientative\n",
    "nuorfdb_df = pd.read_csv(nuORFdb_list, header=None)\n",
    "nuorfdb_list = nuorfdb_df[0].to_list()\n",
    "\n",
    "try:\n",
    "    os.mkdir(DIR+\"/analysis/nuORFdb\")\n",
    "except:\n",
    "    print(\"The directory is not create because it already exists\")\n",
    "    \n",
    "for p in patients_id:\n",
    "    NOCDS=DIR+\"/analysis/09_CIPHER/\"+p+\"/\"+p+\"_NOCDS_id_list_FPKM1.csv\" #from the transcris with coding potential predictred by cipher\n",
    "    OUTNOCDS=DIR+\"/analysis/nuORFdb/\" + p + \"_NOCDS_transcripts_in_db.csv\" #save those that are also considered in nuORFdb\n",
    "    \n",
    "    nocds_df = pd.read_csv(NOCDS, names=['gene_id','transcript_type','transcript_name'], header=None)\n",
    "    shared_nocds = nocds_df[nocds_df.gene_id.isin(nuorfdb_list)]\n",
    "    print(p, \" nocds FPKM1: \",len(shared_nocds))\n",
    "    shared_nocds.to_csv(OUTNOCDS, index=None, header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "and `TransLNC`\n",
    "\n",
    "Dezhong Lv, Zhenghong Chang, Yangyang Cai, Junyi Li, Liping Wang, Qiushuang Jiang, Kang Xu, Na Ding, Xia Li, Juan Xu, Yongsheng Li, TransLnc: a comprehensive resource for translatable lncRNAs extends immunopeptidome, Nucleic Acids Research, Volume 50, Issue D1, 7 January 2022, Pages D413â€“D420, https://doi.org/10.1093/nar/gkab847\n",
    "\n",
    "\n",
    "**TransLnc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$GENOMEDIR\"\n",
    "\n",
    "mkdir $1/translnc\n",
    "cd $1/translnc\n",
    "\n",
    "wget http://bio-bigdata.hrbmu.edu.cn/TransLnc/download/lncRNA_peptide_information.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translnc=os.path.join(GENOMEDIR,\"translnc/lncRNA_peptide_information.txt?x=3\")\n",
    "df = pd.read_csv(translnc, sep=\"\\t\")\n",
    "translnc_identifiers_unique = set(df.iloc[:,9])\n",
    "translnc_identifiers = pd.DataFrame(translnc_identifiers_unique)\n",
    "translnc_identifiers.to_csv(translnc_list_dir, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of the lncRNAs we considered as potentially coding and also did TransLnc. The ORF does not need to be the same. It's just and approximation.\n",
    "translnc_df = pd.read_csv(translnc_list_dir, header=None)\n",
    "translnc_list = translnc_df[0].to_list()\n",
    "\n",
    "try:\n",
    "    os.mkdir(DIR+\"/analysis/translnc\")\n",
    "except:\n",
    "    print(\"The directory is not create because it already exists\")\n",
    "    \n",
    "for p in patients_id:    \n",
    "    OUT=DIR+\"/analysis/translnc/\" + p + \"_lncRNA_transcripts_in_db.csv\"\n",
    "    translnc_df = pd.read_csv(translnc_list_dir, header=None)\n",
    "    translnc_list = translnc_df[0].tolist()\n",
    "    NOCDS=DIR+\"/analysis/09_CIPHER/\"+p+\"/\"+p+\"_NOCDS_id_list_FPKM1.csv\"\n",
    "    NOCDS_df = pd.read_csv(NOCDS, names=['gene_id', 'transcript_type', 'transcript_name'])\n",
    "    NOCDS_df['transcript_name'] = NOCDS_df['transcript_name']\n",
    "    shared_NOCDS = NOCDS_df[NOCDS_df.transcript_name.isin(translnc_list)]\n",
    "\n",
    "    print(p + \" lncRNA FPKM1: \", len(shared_NOCDS))\n",
    "    shared_NOCDS.to_csv(OUT, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which NOCDS selected genes are shared across patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i DIR,patients\n",
    "#generate a file with all lncRNA and the information about whether they are private or shared between patients of the same dataset \n",
    "############PATIENTS INFO#############\n",
    "colnames(patients) <- c(\"patient\", \"normal\", \"tumor\")\n",
    "patients_full <- patients %>% pivot_longer(cols = !patient, names_to = \"normal_tumor\", values_to = \"sample\")\n",
    "\n",
    "all <- data.frame(transcript_id = character(),\n",
    "                  transcript_type = factor(), \n",
    "                  transcript_name = character(),\n",
    "                  patient = factor(),\n",
    "                  stringsAsFactors=FALSE)\n",
    "\n",
    "for (i in 1:nrow(patients)) {\n",
    "  \n",
    "  df <- read.csv(paste0(DIR, \"/analysis/09_CIPHER/\",patients[i,1],\"/\",patients[i,1],\"_NOCDS_id_list_FPKM1.csv\"),header=TRUE)\n",
    "  df$patients <- patients[i,1]\n",
    "  all <- rbind(all,df)\n",
    "}  \n",
    "\n",
    "times <- all %>% count(transcript_id)\n",
    "times_ordered <- times[order(times$n, decreasing=TRUE),]\n",
    "print(times_ordered %>% head)\n",
    "\n",
    "##add gene id\n",
    "t_g_id <- read.csv(\"/projects_eg/projects/marta/biomart.v38.geneid_transid.csv\")[,c('Gene_stable_ID', 'Transcript_stable_ID', 'Gene_name')]\n",
    "names(t_g_id) <- c(\"gene_id\",\"transcript_id\",\"gene_name\")\n",
    "total <- merge(times_ordered,t_g_id, by=\"transcript_id\")\n",
    "total <- total[,c(1,3,4,2)]\n",
    "total_ordered <- total[order(total$n, decreasing=TRUE),]\n",
    "total_ordered <- total_ordered %>% distinct()\n",
    "\n",
    "write.csv(total_ordered, file.path(DIR,\"analysis/09_CIPHER/common_noncoding_genes.csv\"),row.names = FALSE, quote = FALSE)\n",
    "\n",
    "ggplot(times, aes(x=n)) +\n",
    "  geom_bar(fill=\"#004D40\") +\n",
    "  geom_text(stat=\"count\", aes(label=..count..), vjust=-1, size = 2.5) +\n",
    "ggtitle(\"Common non-coding genes across patients | lncRNA & processed_pseudogenes\") +\n",
    "  theme(axis.title.x = element_blank(), axis.title.y = element_blank(), plot.title = element_text(face=\"bold\", size = 9), legend.position = \"none\") \n",
    "ggsave(file.path(DIR,\"results/plots/common_noncoding_genes_patients.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel genes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DIR\" \"$patients_summary\" \"$GENOMEDIR\"\n",
    "\n",
    "export PATH=/genomics/users/marta/tools/gffread-0.12.7.Linux_x86_64/:$PATH\n",
    "GENOME_FASTA=$3/GRCh38/GRCh38.primary_assembly.genome.fa\n",
    "\n",
    "cat $2 | while IFS=, read p normal tumor; do\n",
    "    file=$1/analysis/08_tumor_specific/${p}/${p}_novel_tumor_specific_genes_1FPKM_300kb.gtf\n",
    "    #get fasta. Little and not informative header is generated\n",
    "    gffread -w ${file%%.*}.fa -g $GENOME_FASTA $file\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#an informative heaer with the coodinates and chromosome is added\n",
    "for p in patients_id:\n",
    "    print(p)\n",
    "    INDIR=DIR+\"/analysis/08_tumor_specific/\"+p\n",
    "    try:\n",
    "        df = read_gtf(INDIR+\"/\" + p + \"_novel_tumor_specific_genes_1FPKM_300kb.gtf\")\n",
    "        t_df = df.loc[df['feature'] == \"transcript\"]\n",
    "        g_id = list(t_df['gene_id'])\n",
    "    except:\n",
    "        print(\"%s patients has no novel genes\" %(p))\n",
    "\n",
    "    for file in os.listdir(DIR):\n",
    "        if \"novel\" in file and file.endswith(\".fa\"):\n",
    "\n",
    "            full_file = os.path.join(DIR,file)\n",
    "            outname = file[:-3] + \"_fullheader.fa\"\n",
    "            out = os.path.join(DIR,outname)\n",
    "            with open(out, 'w') as outfile:\n",
    "                for seq_record in SeqIO.parse(full_file, 'fasta'):\n",
    "                    identifier = str(seq_record.id)\n",
    "                    name=identifier[:-2]\n",
    "                    #if identifier.startswith(\"STRG\"):\n",
    "                    #    name=re.findall(r\"[^.]*.[^.]*\", identifier)[0]\n",
    "                    row_index = g_id.index(name)\n",
    "                    info_to_add = t_df.iloc[row_index,[0,3,4]]\n",
    "                    listToStr = ','.join(map(str, info_to_add))\n",
    "                    outfile.write(\">%s,%s\\n%s\\n\" %(identifier,listToStr,str(seq_record.seq)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIPHER on NOVEL GENES**\n",
    "\n",
    "Both strands are being considered, since we do not have information about the strand for novel genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$DIR\" \"$patients_summary\"\n",
    "module load Python/2.7.11\n",
    "\n",
    "cat $2 | while IFS=, read p normal tumor; do\n",
    "    INDIR=$1/analysis/08_tumor_specific/$p\n",
    "    OUTPUT=$1/analysis/09_CIPHER/$p\n",
    "\n",
    "    #predict the longest ORF\n",
    "    python /genomics/users/marta/tools/CIPHER-master/cipher.py -i $INDIR/${p}_novel_tumor_specific_genes_1FPKM_300kb_fullheader.fa -o $OUTPUT/${p}_novel_tumor_specific_genes_1FPKM_300kb_8aa_cipher_longest -s human -x /genomics/users/marta/tools/CIPHER-master/tables/hsa_coding_to_intron_dicodon_usage.obj -t 8\n",
    "    #predict all ORFs\n",
    "    python /genomics/users/marta/tools/CIPHER-master/cipher.py -n all -i $INDIR/${p}_novel_tumor_specific_genes_1FPKM_300kb_fullheader.fa -o $OUTPUT/${p}_novel_tumor_specific_genes_1FPKM_300kb_8aa_cipher_all -s human -x /genomics/users/marta/tools/CIPHER-master/tables/hsa_coding_to_intron_dicodon_usage.obj -t 8\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get reverse strand sequence\n",
    "for p in patients_id:\n",
    "    file=DIR+\"/analysis/08_tumor_specific/\"+p+\"/\"+p+\"_novel_tumor_specific_genes_1FPKM_300kb.fa\"\n",
    "    output=DIR+\"/analysis/08_tumor_specific/\"+p+\"/\"+p+\"_novel_tumor_specific_genes_1FPKM_300kb_REVERSE.fa\"\n",
    "    with open(output, 'w') as out:\n",
    "        for seq_record in SeqIO.parse(file, 'fasta'):\n",
    "            out.write(\">%s\\n%s\\n\" %(seq_record.id, seq_record.seq.complement()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add coordinates to the header. Since it's reversed strand, the coordinates are not well added. They give an idea of the paired forward gene\n",
    "for p in patients_id:\n",
    "    print(p)\n",
    "    INDIR=DIR+\"/analysis/08_tumor_specific/\"+p+\"/\"\n",
    "    try:\n",
    "        df = read_gtf(INDIR+ p + \"_novel_tumor_specific_genes_1FPKM_300kb.gtf\")\n",
    "        t_df = df.loc[df['feature'] == \"transcript\"]\n",
    "        g_id = list(t_df['gene_id'])\n",
    "    except:\n",
    "        print(\"%s patients has no novel genes\" %(p))\n",
    "\n",
    "    for file in os.listdir(INDIR):\n",
    "        if \"novel\" in file and file.endswith(\"REVERSE.fa\"):\n",
    "\n",
    "            full_file = os.path.join(INDIR,file)\n",
    "            outname = file[:-3] + \"_fullheader.fa\"\n",
    "            out = os.path.join(INDIR,outname)\n",
    "            with open(out, 'w') as outfile:\n",
    "                for seq_record in SeqIO.parse(full_file, 'fasta'):\n",
    "                    identifier = str(seq_record.id)\n",
    "                    name=identifier[:-2]\n",
    "                    #if identifier.startswith(\"STRG\"):\n",
    "                    #    name=re.findall(r\"[^.]*.[^.]*\", identifier)[0]\n",
    "                    row_index = g_id.index(name)\n",
    "                    info_to_add = t_df.iloc[row_index,[0,3,4]]\n",
    "                    listToStr = ','.join(map(str, info_to_add))\n",
    "                    outfile.write(\">R%s,%s\\n%s\\n\" %(identifier,listToStr,str(seq_record.seq)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DIR\" \"$patients_summary\"\n",
    "module load Python/2.7.11\n",
    "\n",
    "cat $2 | while IFS=, read p normal tumor; do\n",
    "    INDIR=$1/analysis/08_tumor_specific/$p\n",
    "    OUTPUT=$1/analysis/09_CIPHER/$p\n",
    "    \n",
    "    #predict the longest ORF\n",
    "    python /genomics/users/marta/tools/CIPHER-master/cipher.py -i $INDIR/${p}_novel_tumor_specific_genes_1FPKM_300kb_REVERSE_fullheader.fa -o $OUTPUT/${p}_novel_tumor_specific_genes_1FPKM_300kb_REVERSE_8aa_cipher_longest -s human -x /genomics/users/marta/tools/CIPHER-master/tables/hsa_coding_to_intron_dicodon_usage.obj -t 8\n",
    "    #predict all ORFs\n",
    "    python /genomics/users/marta/tools/CIPHER-master/cipher.py -n all -i $INDIR/${p}_novel_tumor_specific_genes_1FPKM_300kb_REVERSE_fullheader.fa -o $OUTPUT/${p}_novel_tumor_specific_genes_1FPKM_300kb_REVERSE_8aa_cipher_all -s human -x /genomics/users/marta/tools/CIPHER-master/tables/hsa_coding_to_intron_dicodon_usage.obj -t 8\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all non-canonical information (both lncRNA/processed pseudogenes(NOCDS selected) and novel genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$DIR\" \"$patients_summary\"\n",
    "if [ -f \"${INDIR}/${p}_noncanonical_all_orfs_FPKM1.fa\" ] ; then\n",
    "    rm \"${INDIR}/${p}_noncanonical_all_orfs_FPKM1.fa\"\n",
    "fi\n",
    "cat $2 | while IFS=, read p normal tumor; do\n",
    "    INDIR=$1/analysis/09_CIPHER/${p}\n",
    "    listfiles=\"${p}_novel_tumor_specific_genes_1FPKM_300kb_8aa_cipher_all_orfs.fa ${p}_novel_tumor_specific_genes_1FPKM_300kb_REVERSE_8aa_cipher_all_orfs.fa ${p}_known_tumor_specific_genes_1FPKM_300kb_NOCDS_selected_8aa_cipher_all_orfs.fa\"\n",
    "\n",
    "    for file in $listfiles; do\n",
    "        cat $INDIR/$file >> ${INDIR}/${p}_noncanonical_all_orfs_FPKM1.fa #generate a file with all the novel open reading frames\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To consider all potential ORFs can generate some redundancies we are not interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove ORF redundancies. If an ORF is inside another one under the same identifier, the nested is removed.\n",
    "for p in patients_id:\n",
    "    fasta_file=DIR+\"/analysis/09_CIPHER/\" + p + \"/\" + p + \"_noncanonical_all_orfs_FPKM1.fa\"\n",
    "    fasta_dict = dict()\n",
    "    IDs= []\n",
    "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):\n",
    "        \n",
    "        if seq_record.id.startswith('ENST'):\n",
    "            identifier=seq_record.id[:-2]\n",
    "            #identifier=re.findall(r\"[^.]*\", seq_record.id)[0]\n",
    "            if identifier not in IDs:\n",
    "                IDs.append(identifier)\n",
    "                fasta_dict[identifier] = {}\n",
    "            if identifier in IDs:\n",
    "                orf=re.findall(r\".*transcript_name=.*_(.*)\", seq_record.id)[0]\n",
    "                fasta_dict[str(identifier)][orf] = str(seq_record.seq)\n",
    "            #print(identifier)\n",
    "        elif seq_record.id.startswith('STRG') or seq_record.id.startswith('RSTRG'):\n",
    "            identifier=seq_record.id.split(\"_\")[0]\n",
    "            if identifier not in IDs:\n",
    "                IDs.append(identifier)\n",
    "                fasta_dict[identifier] = {}\n",
    "            if identifier in IDs:\n",
    "                orf=seq_record.id.split(\"_\")[1]\n",
    "                fasta_dict[str(identifier)][orf] = str(seq_record.seq)\n",
    "            #print(identifier)\n",
    "\n",
    "    clean_dict = {}\n",
    "    duplis = {}\n",
    "    total = 0\n",
    "\n",
    "    length_to_delete = 0\n",
    "    summary_out=DIR+\"/results/predicted_ORF.txt\"\n",
    "    with open(summary_out, 'w') as summary:\n",
    "        with open(DIR+\"/analysis/09_CIPHER/\" + p +\"/id_num_orfs.txt\",'w') as num_orfs: # file with the number of orfs per identifier\n",
    "            for item in IDs:\n",
    "                individual_dict = fasta_dict[item]\n",
    "                total = total + len(individual_dict)\n",
    "                num_orfs.write('%s\\t%s\\n' %(item,len(individual_dict)))\n",
    "\n",
    "                clean_dict[item] = {}\n",
    "                if len(individual_dict) == 1:\n",
    "                    clean_dict[item] = individual_dict\n",
    "                    next\n",
    "\n",
    "                else:\n",
    "                    count = 0\n",
    "                    to_delete = []\n",
    "                    for orf, initial_seq in individual_dict.items():\n",
    "                        values=list(individual_dict.values())\n",
    "                        for seq in values:\n",
    "                            if values[count] in seq and values[count] != seq:\n",
    "                                name=item + \"_\" + orf\n",
    "                                to_delete.append(values[count])\n",
    "                                duplis[name] = values[count]\n",
    "                        count += 1\n",
    "\n",
    "                    length_to_delete =length_to_delete + len(to_delete)\n",
    "                    myDict = {key:val for key, val in individual_dict.items() if val not in to_delete}\n",
    "                    clean_dict[item] = myDict\n",
    "        summary.write(p)\n",
    "        summary.write(\"initial: \"+str(total))\n",
    "\n",
    "        DUPLIS=DIR+\"/analysis/09_CIPHER/\"+p+\"/\" + p + \"_noncanonical_all_orfs_FPKM1_duplis.fa\"\n",
    "        with open(DUPLIS, 'w') as d:\n",
    "            summary.write(\"duplis: \"+str(len(duplis)))\n",
    "            for k, v in duplis.items():\n",
    "                d.write(\">%s\\n%s\\n\" %(k,v))\n",
    "\n",
    "        OUT=DIR+\"/analysis/09_CIPHER/\" + p + \"/\" + p + \"_noncanonical_all_orfs_FPKM1_notduplis.fa\"\n",
    "        with open(OUT, 'w') as out:\n",
    "            length = 0\n",
    "            for k, v in clean_dict.items():\n",
    "                length = length + len(v)\n",
    "                for orf, seq in v.items():\n",
    "                    out.write('>%s_%s\\n%s\\n' %(k,orf,seq))\n",
    "            summary.write(\"norepetead: \"+str(length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in patients_id:\n",
    "    identifiers= []\n",
    "    INDIR=DIR+\"/analysis/09_CIPHER/\" + p \n",
    "    filename= p + \"_noncanonical_all_orfs_FPKM1.fa\"\n",
    "    file = os.path.join(INDIR, filename)\n",
    "    outname= p + \"_noncanonical_id_list_FPKM1.csv\" #list with all the noncanonical orfs\n",
    "    outfile= os.path.join(INDIR,outname)\n",
    "    with open(outfile, 'w') as out:\n",
    "        with open(file) as fasta_file:  \n",
    "            for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)  \n",
    "                if seq_record.id.startswith(\"STRG\") or seq_record.id.startswith(\"RSTRG\"):\n",
    "                    name=re.findall(r\"[^.]*.[^.]*\", seq_record.id)[0]\n",
    "                    identifiers.append(name)\n",
    "                else:\n",
    "                    identifiers.append(seq_record.id)\n",
    "\n",
    "        to_write = set(identifiers)\n",
    "        for i in to_write:\n",
    "            if i.startswith(\"ENST\"):\n",
    "                out.write(i[0:15]+\"\\n\")\n",
    "            else:\n",
    "                out.write(i + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translate the ORFs into potential protein sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in patients_id:\n",
    "    INDIR=DIR+\"/analysis/09_CIPHER/\"+p\n",
    "\n",
    "    file= p + \"_noncanonical_all_orfs_FPKM1_notduplis.fa\"\n",
    "    outfile = file[:-3] + \"_PROTEIN.fa\"\n",
    "    with open (os.path.join(INDIR,outfile), 'w') as out:\n",
    "        for seq_record in SeqIO.parse(os.path.join(INDIR,file), \"fasta\"):\n",
    "            identif=str(seq_record.id)\n",
    "            sequence=str(seq_record.seq.translate())\n",
    "            out.write(\">\" + identif + \"\\n\" + sequence[:-1] + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
